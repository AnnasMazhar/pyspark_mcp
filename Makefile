# PySpark Tools - Docker-based Testing Makefile
# This Makefile provides convenient commands for running tests in Docker

.PHONY: help build test test-all test-module test-coverage test-performance test-integration
.PHONY: test-sql-converter test-batch-processor test-duplicate-detector test-file-utils test-server
.PHONY: quality lint format clean cleanup watch

# Default target
help: ## Show this help message
	@echo "PySpark Tools - Docker Testing Commands"
	@echo ""
	@echo "Usage: make [TARGET]"
	@echo ""
	@echo "Targets:"
	@awk 'BEGIN {FS = ":.*?## "} /^[a-zA-Z_-]+:.*?## / {printf "  %-20s %s\n", $$1, $$2}' $(MAKEFILE_LIST)

# Build targets
build: ## Build Docker image (cached)
	@echo "ğŸ”¨ Building Docker image..."
	docker-compose build

build-clean: ## Build Docker image with no cache (for clean builds)
	@echo "ğŸ”¨ Building Docker image (no cache)..."
	docker-compose build --no-cache

# Test targets
test: test-quick ## Run quick tests (alias for test-quick)

test-all: build ## Run comprehensive test suite with coverage
	@echo "ğŸ§ª Running all tests with coverage..."
	./test_runner.sh all

test-quick: ## Run quick tests without rebuild (FAST)
	@echo "âš¡ Running quick tests..."
	docker-compose --profile test run --rm pyspark-tools-test pytest tests/test_advanced_optimizer.py -v --tb=short

test-coverage: build ## Run tests with detailed coverage analysis
	@echo "ğŸ“Š Running coverage analysis..."
	./test_runner.sh coverage

# Module-specific testing
test-module: ## Run tests for specific module (usage: make test-module MODULE=sql-converter)
	@if [ -z "$(MODULE)" ]; then \
		echo "âŒ MODULE parameter required. Usage: make test-module MODULE=sql-converter"; \
		echo "Available modules: sql-converter, batch-processor, duplicate-detector, file-utils, server"; \
		exit 1; \
	fi
	@echo "ğŸ”¬ Testing module: $(MODULE)"
	./test_runner.sh module $(MODULE)

test-sql-converter: ## Run SQL converter module tests (FAST)
	@echo "ğŸ”¬ Testing SQL converter module..."
	docker-compose --profile test run --rm test-sql-converter

test-batch-processor: ## Run batch processor module tests (FAST)
	@echo "ğŸ”¬ Testing batch processor module..."
	docker-compose --profile test run --rm test-batch-processor

test-duplicate-detector: ## Run duplicate detector module tests (FAST)
	@echo "ğŸ”¬ Testing duplicate detector module..."
	docker-compose --profile test run --rm test-duplicate-detector

test-file-utils: ## Run file utilities module tests (FAST)
	@echo "ğŸ”¬ Testing file utilities module..."
	docker-compose --profile test run --rm test-file-utils

test-server: ## Run MCP server module tests (FAST)
	@echo "ğŸ”¬ Testing MCP server module..."
	docker-compose --profile test run --rm test-server

test-advanced-optimizer: ## Run advanced optimizer module tests (FAST)
	@echo "ğŸ”¬ Testing advanced optimizer module..."
	docker-compose --profile test run --rm pyspark-tools-test pytest tests/test_advanced_optimizer.py -v --tb=short

test-data-source-analyzer: ## Run data source analyzer module tests (FAST)
	@echo "ğŸ”¬ Testing data source analyzer module..."
	docker-compose --profile test run --rm pyspark-tools-test pytest tests/test_data_source_analyzer.py -v --tb=short

# Specialized testing
test-performance: build ## Run performance tests
	@echo "âš¡ Running performance tests..."
	./test_runner.sh performance

benchmark: build ## Run comprehensive performance benchmarks
	@echo "ğŸ¯ Running performance benchmarks..."
	docker-compose --profile test run --rm pyspark-tools-test python scripts/benchmark_performance.py

validate-performance: build ## Validate performance against v1.0 targets
	@echo "âœ… Validating performance targets..."
	docker-compose --profile test run --rm pyspark-tools-test bash scripts/validate_performance.sh

test-integration: build ## Run integration tests
	@echo "ğŸ”— Running integration tests..."
	./test_runner.sh integration

test-specific: ## Run specific test file (usage: make test-specific FILE=test_sql_converter.py)
	@if [ -z "$(FILE)" ]; then \
		echo "âŒ FILE parameter required. Usage: make test-specific FILE=test_sql_converter.py"; \
		exit 1; \
	fi
	@echo "ğŸ¯ Running specific test: $(FILE)"
	./test_runner.sh specific $(FILE)

# Quality targets
quality: build ## Run code quality checks only
	@echo "âœ¨ Running quality checks..."
	./test_runner.sh quality

lint: ## Run linting checks in Docker
	@echo "ğŸ” Running linting..."
	docker-compose run --rm pyspark-tools-test flake8 pyspark_tools/ tests/ run_server.py

format: ## Format code using black and isort
	@echo "ğŸ¨ Formatting code..."
	docker-compose run --rm pyspark-tools-test python -m black pyspark_tools/ tests/ run_server.py
	docker-compose run --rm pyspark-tools-test python -m isort pyspark_tools/ tests/ run_server.py

# Development targets
watch: build ## Run tests in watch mode (for development)
	@echo "ğŸ‘€ Starting watch mode..."
	./test_runner.sh watch

# Utility targets
clean: ## Clean up Docker resources
	@echo "ğŸ§¹ Cleaning up..."
	./test_runner.sh cleanup

cleanup: clean ## Alias for clean

# Docker compose shortcuts
up: ## Start the main application service
	@echo "ğŸš€ Starting PySpark Tools server..."
	docker-compose up -d pyspark-tools

down: ## Stop all services
	@echo "ğŸ›‘ Stopping services..."
	docker-compose down

logs: ## Show logs from the main service
	@echo "ğŸ“‹ Showing logs..."
	docker-compose logs -f pyspark-tools

# Advanced testing combinations
test-no-build: ## Run tests without rebuilding (FASTEST)
	@echo "âš¡ Running tests without rebuild..."
	docker-compose --profile test run --rm pyspark-tools-test pytest tests/ -v --tb=short -x

test-slow: ## Run slow tests (integration and performance)
	@echo "ğŸŒ Running slow tests..."
	docker-compose --profile test run --rm pyspark-tools-test pytest tests/ -v -m "integration or performance" --tb=short

test-unit: ## Run unit tests only
	@echo "ğŸ”¬ Running unit tests..."
	docker-compose --profile test run --rm pyspark-tools-test pytest tests/ -v -m "unit" --tb=short

# Parallel testing
test-parallel: build ## Run tests in parallel (faster execution)
	@echo "âš¡ Running tests in parallel..."
	docker-compose --profile test run --rm pyspark-tools-test pytest tests/ -v -n auto --tb=short

# Coverage targets
coverage-html: build ## Generate HTML coverage report
	@echo "ğŸ“Š Generating HTML coverage report..."
	mkdir -p coverage_reports
	docker-compose --profile test run --rm pyspark-tools-test pytest tests/ --cov=pyspark_tools --cov-report=html:/app/coverage_html
	@echo "âœ… Coverage report available in coverage_reports/index.html"

coverage-xml: build ## Generate XML coverage report (for CI)
	@echo "ğŸ“Š Generating XML coverage report..."
	docker-compose --profile test run --rm pyspark-tools-test pytest tests/ --cov=pyspark_tools --cov-report=xml:/app/coverage.xml

# CI/CD targets
ci-test: ## Run tests suitable for CI environment
	@echo "ğŸ¤– Running CI tests..."
	docker-compose build --no-cache
	docker-compose --profile test run --rm pyspark-tools-test pytest tests/ -v --cov=pyspark_tools --cov-report=term --cov-report=xml --tb=short

# Development helpers
shell: ## Open shell in test container
	@echo "ğŸš Opening shell in test container..."
	docker-compose --profile test run --rm pyspark-tools-test bash

debug: ## Run tests with debugging enabled
	@echo "ğŸ› Running tests with debugging..."
	docker-compose --profile test run --rm pyspark-tools-test pytest tests/ -v -s --tb=long --pdb

# Documentation
test-docs: ## Test documentation examples
	@echo "ğŸ“š Testing documentation examples..."
	docker-compose --profile test run --rm pyspark-tools-test pytest tests/ -v -k "doc" --tb=short

# Security testing
test-security: ## Run security-focused tests
	@echo "ğŸ”’ Running security tests..."
	docker-compose --profile test run --rm pyspark-tools-test pytest tests/ -v -k "security" --tb=short

security-audit: build ## Run comprehensive security audit
	@echo "ğŸ” Running security audit..."
	docker-compose --profile test run --rm pyspark-tools-test python scripts/security_audit.py

security-scan: ## Run dependency vulnerability scan
	@echo "ğŸ” Scanning for vulnerabilities..."
	docker-compose --profile test run --rm pyspark-tools-test python -c "import subprocess; subprocess.run(['python', '-m', 'pip', 'install', 'safety'], check=False); subprocess.run(['safety', 'check', '--file', 'requirements.txt'])" || echo "âš ï¸  Safety scan completed with warnings"

# Release targets
tag-release: ## Tag a new release (usage: make tag-release VERSION=1.0.0)
	@if [ -z "$(VERSION)" ]; then \
		echo "âŒ VERSION parameter required. Usage: make tag-release VERSION=1.0.0"; \
		exit 1; \
	fi
	@echo "ğŸ·ï¸  Tagging release v$(VERSION)..."
	./scripts/tag_release.sh $(VERSION)

prepare-release: ## Prepare release (run all quality checks)
	@echo "ğŸš€ Preparing release..."
	make build-clean
	make test-all
	make test-security
	make test-performance
	make coverage-html
	@echo "âœ… Release preparation complete!"

validate-release: ## Validate release readiness
	@echo "âœ… Validating release readiness..."
	@echo "ğŸ“‹ Checking required files..."
	@test -f CHANGELOG.md || (echo "âŒ CHANGELOG.md missing" && exit 1)
	@test -f LICENSE || (echo "âŒ LICENSE missing" && exit 1)
	@test -f README.md || (echo "âŒ README.md missing" && exit 1)
	@test -f pyproject.toml || (echo "âŒ pyproject.toml missing" && exit 1)
	@test -f requirements.txt || (echo "âŒ requirements.txt missing" && exit 1)
	@test -f Dockerfile || (echo "âŒ Dockerfile missing" && exit 1)
	@echo "ğŸ“Š Checking version consistency..."
	@grep -q "version = " pyproject.toml || (echo "âŒ Version not found in pyproject.toml" && exit 1)
	@echo "ğŸ§ª Running final quality checks..."
	make lint
	make test-all
	@echo "âœ… Release validation passed!"

publish-pypi: ## Publish to PyPI (requires authentication)
	@echo "ğŸ“¦ Publishing to PyPI..."
	docker-compose run --rm pyspark-tools-test python -m build
	docker-compose run --rm pyspark-tools-test python -m twine upload dist/*

docker-push: ## Push Docker image to registry
	@echo "ğŸ³ Pushing Docker image..."
	@if [ -z "$(VERSION)" ]; then \
		echo "âŒ VERSION parameter required. Usage: make docker-push VERSION=1.0.0"; \
		exit 1; \
	fi
	docker tag pyspark-tools:latest pyspark-tools:$(VERSION)
	docker push pyspark-tools:$(VERSION)
	docker push pyspark-tools:latest

validate-production: ## Validate production deployment readiness
	@echo "ğŸ­ Validating production deployment readiness..."
	@echo "ğŸ“‹ This will run comprehensive validation including:"
	@echo "   â€¢ Docker build and container startup"
	@echo "   â€¢ MCP endpoint functionality"
	@echo "   â€¢ Real-world usage scenarios"
	@echo "   â€¢ Resource usage under load"
	@echo "   â€¢ Configuration security"
	@echo ""
	@read -p "Continue with production validation? [y/N] " confirm && [ "$$confirm" = "y" ]
	python scripts/validate_production_deployment.py

production-ready: ## Complete production readiness validation
	@echo "ğŸ¯ Running complete production readiness validation..."
	make validate-release
	make validate-performance
	make security-audit
	make validate-production
	@echo ""
	@echo "âœ… Production readiness validation complete!"
	@echo "ğŸ“‹ Review PRODUCTION_READINESS.md for final checklist"